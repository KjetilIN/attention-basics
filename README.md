# Attention Basics

Learning attention mechanism from scratch. 


## Usage

Work with the notebooks by creating a virtual environment and use the `requirements.txt` to setup the packages.

## Resources

"Attention is all you need" paper: <br>

```bibtex
@misc{vaswani2023attentionneed,
    title={Attention Is All You Need}, 
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2023},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/1706.03762}, 
}
```


Pytorch help from Luke Ditria: <br>
https://github.com/LukeDitria/pytorch_tutorials 